[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to the Teaching Lab blog, where data science and education converge to inspire transformative teaching and learning. Our data scientists and educators dive deep into data to uncover insights that shape effective, equitable teaching strategies. From student performance analyses to innovative educational tools, we explore how data can inform and revolutionize education. Join us for cutting-edge research, case studies, and practical tips to empower your teaching. Dive into the future of education with us.\n\n\n\n\n\n\n\n\n\n\nData Quality At Teaching Lab: Definitions, Examples, and Best Practices\n\n\n\n\n\n\nequity\n\n\n\nData quality is defined as the degree to which data meets a company‚Äôs expectations of accuracy, validity, completeness, and consistency.\n\n\n\n\n\nJul 15, 2024\n\n\nLauren Hartmann\n\n\n\n\n\n\n\n\n\n\n\n\nAdvancing Data Equity: Progress and Future Strategies\n\n\n\n\n\n\nequity\n\n\n\nTeaching Lab is committed to minimizing bias in research through our Data Equity Plan. We‚Äôve prioritized stakeholder feedback, improved survey accessibility, and educated our team on data equity to ensure diverse experiences are represented and equity goals are met.\n\n\n\n\n\nJun 24, 2024\n\n\nMary Kate Dykes\n\n\n\n\n\n\n\n\n\n\n\n\nVoices from the Classroom: Student Survey Findings from a Rigorous Research Study in New Mexico\n\n\n\n\n\n\nmetrics\n\n\nbusiness growth\n\n\n\nPreliminary results from the New Mexico RCT indicate that Teaching Lab PL positively impacts students‚Äô learning experiences. Students whose teachers had participated in PL reported more positive perceptions of their teachers‚Äô use of culturally responsive teaching methods in addition to greater self-confidence in their mathematical skills, compared to students whose teachers had not yet participated in PL.\n\n\n\n\n\nMay 8, 2024\n\n\nChing Sullivan and Mary Kate Dykes\n\n\n\n\n\n\n\n\n\n\n\n\nThe Basics: What is NPS?\n\n\n\n\n\n\nmetrics\n\n\nbusiness growth\n\n\n\nExploring the Net Promoter Score (NPS), this blog post unpacks its role as a key metric for assessing customer loyalty and predicting business growth, despite its critiques and complexities. We delve into why Teaching Lab adopts NPS, its advantages, and the nuanced challenges it presents. Through a balanced examination, we present NPS as a valuable, though not flawless, tool in the broader landscape of customer feedback.\n\n\n\n\n\nMar 15, 2024\n\n\nDuncan Gates\n\n\n\n\n\n\n\n\n\n\n\n\nElevating Teacher Growth: Insights into Desired Professional Learning Experiences\n\n\n\n\n\n\npl preferences\n\n\npd format\n\n\n\nIn the ever-evolving landscape of education, the quest for impactful professional learning (PL) experiences is paramount for teachers seeking to enhance their pedagogical skills and classroom effectiveness. Dr.¬†Omar Headen delves into the heart of what educators truly desire from PL opportunities, drawing from comprehensive survey data collected over the 2022-2023 school year.\n\n\n\n\n\nFeb 22, 2024\n\n\nOmar Headen\n\n\n\n\n\n\n\n\n\n\n\n\nChanging the Game: How High-Dosage Coaching Shifts Teacher Practices & Student Success\n\n\n\n\n\n\ncoaching\n\n\nmindsets\n\n\n\nDiscover how Teaching Lab‚Äôs innovative coaching is transforming classrooms by uplifting both teachers and students.\n\n\n\n\n\nDec 15, 2023\n\n\nShaye Worthman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Dashboards.html#participant-perceptions",
    "href": "Dashboards.html#participant-perceptions",
    "title": "Dashboards",
    "section": "Participant Perceptions",
    "text": "Participant Perceptions\nParticipant perceptions include data from the end of session survey administered at the end of each PL session, end of course survey administered at the end of each PL course, ongoing coaching survey administered at the end of each coaching session, and end of coaching data, administered at the end of every coaching series. Lastly, NPS data is also prompted in this survey.\n\n\n\n\n\n\n\n\n\n\nParticipant Feedback // Ongoing Report\n\n\n\nParticipant Feedback\n\n\nNPS\n\n\nQualitative\n\n\n\nView ALL participant feedback data in the overall form including quantitative and qualitative feedback, and all NPS in section 1 of the ongoing report.\n\n\n\nDuncan Gates\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParticipant Feedback Rollup\n\n\n\nParticipant Feedback\n\n\nNPS\n\n\nOver Time\n\n\nAI\n\n\n\nView all participant feedback data with facilitator competency feedback included in a more detailed breakdown. Oriented towards considering specific facilitator feedback as‚Ä¶\n\n\n\nDuncan Gates\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParticipant Feedback Qualtrics Dashboard\n\n\n\nQualtrics\n\n\n\nA qualtrics dashboard for participant feedback\n\n\n\nDuncan Gates\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Dashboards.html#participant-mindsets",
    "href": "Dashboards.html#participant-mindsets",
    "title": "Dashboards",
    "section": "Participant Mindsets",
    "text": "Participant Mindsets\nParticipant mindsets come from the diagnostic educator survey and follow up educator survey.\n\n\n\n\n\n\n\n\n\n\nEducator Survey // Ongoing Report\n\n\n\nEducator Survey\n\n\nMindsets\n\n\nCRSE\n\n\n\nSee teacher mindset data in section 3a, or school leader mindsets in the alternative 3a selection, and CRSE data in section 3c.\n\n\n\nDuncan Gates\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEducator Survey Qualtrics Data\n\n\n\nQualtrics\n\n\n\nView educator survey data in qualtrics.\n\n\n\nDuncan Gates\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Dashboards.html#participant-knowledge",
    "href": "Dashboards.html#participant-knowledge",
    "title": "Dashboards",
    "section": "Participant Knowledge",
    "text": "Participant Knowledge\nParticipants that completed course assessments will answer multiple choice quiz questions that address pedagogical content knowledge.\n\n\n\n\n\n\n\n\n\n\nCourse Assessments Dashboard\n\n\n\nCourse Assessments\n\n\nMindsets\n\n\n\nView course assessments data as % correct before/after, % correct by each question, and finally % selected within each question!\n\n\n\nDuncan Gates\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCourse Assessments // Ongoing Report\n\n\n\nCourse Assessments\n\n\nMindsets\n\n\n\nView course assessments overall pre-post scores in section 2 the ongoing report!\n\n\n\nDuncan Gates\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Dashboards.html#participant-practices",
    "href": "Dashboards.html#participant-practices",
    "title": "Dashboards",
    "section": "Participant Practices",
    "text": "Participant Practices\nTeaching Lab staff conduct classroom observations and capture standards and shifts aligned instruction in the classroom observations survey. Those results will appear in the following dashboards.\n\n\n\n\n\n\n\n\n\n\nIPG Raw Data/Downloadable Dashboard\n\n\n\nIPG\n\n\nRaw Data\n\n\nClassroom Observations\n\n\n\nDownload classroom observations data in a rolled up summative form, or view entire responses on a per-teacher basis in this interactive dashboard\n\n\n\nDuncan Gates\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIPG Ongoing Report Data\n\n\n\nClassroom Observations\n\n\nIPG\n\n\n\nView % positive indicators over the course of the year in section 4a, 4b, and 4c, and select specific ipg rubrics to see improvements in core action over time.\n\n\n\nDuncan Gates\n\n\nFeb 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle Sheets Raw Copy of Classroom Observations\n\n\n\nQualtrics\n\n\nRaw Data\n\n\nGoogle Sheets\n\n\n\nSee a raw spreadsheet copy of all submitted IPG data, each tab contains responses from a specific IPG rubric.\n\n\n\nDuncan Gates\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIPG Qualtrics Dashboard\n\n\n\nQualtrics\n\n\n\nClassroom Observations Data in Qualtrics\n\n\n\nDuncan Gates\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Dashboards.html#student-learning-experiences",
    "href": "Dashboards.html#student-learning-experiences",
    "title": "Dashboards",
    "section": "Student Learning Experiences",
    "text": "Student Learning Experiences\nRespondents to the student survey and submissions of student work samples have their responses rolled up and graded in the following dashboards.\n\n\n\n\n\n\n\n\n\n\nStudent Work Graded Dashboard\n\n\n\nStudent Work\n\n\n\nView graded student work, remaining files, contest grades, and see the % proficient and on grade level after assignments have been submitted and graded in the above‚Ä¶\n\n\n\nDuncan Gates\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudent Work Grading Web App\n\n\n\nStudent Work\n\n\n\nGrade student work assignments that have been submitted to Teaching Lab! For approval to grade please reach out to Duncan Gates.\n\n\n\nDuncan Gates\n\n\nFeb 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudent Outcomes // Ongoing Report\n\n\n\nStudent Survey\n\n\n\nView % agree/strongly agree on the student survey in section 5a (student survey & student work).\n\n\n\nDuncan Gates\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dashboard/participant_feedback_2.html",
    "href": "dashboard/participant_feedback_2.html",
    "title": "Participant Feedback Rollup",
    "section": "",
    "text": "[Go to the facilitator feedback report ‚û°Ô∏è]https://tl-data.teachinglab.org/shiny/tl_facilitator_data)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "On this website you will see information about Teaching Lab data according to your role. At the moment you are signed in as an [coach_facilitator_external_internal] user."
  },
  {
    "objectID": "blog/02_beyond_the_basics/index.html",
    "href": "blog/02_beyond_the_basics/index.html",
    "title": "Elevating Teacher Growth: Insights into Desired Professional Learning Experiences",
    "section": "",
    "text": "Have you ever attended a professional learning (PL) session and walked away feeling disconnected? Across the country, there needs to be more clarity between what is commonly offered to teachers and what they really want from PL. A school‚Äôs success depends on designing successful, meaningful, innovative PL that kindles teachers‚Äô passion for continuous improvement.¬†\n\nNaturally, teachers want to invest their time and effort in activities that are relevant, practical, and directly applicable to their classrooms. In this blog post, we provide a¬†snapshot of PL opportunities some of our partners are eager to invest their time and effort in according to Teaching Lab survey data.\n\nOver the 2022-2023 school year, 1600 teachers indicated their preferences for upcoming professional learning opportunities. Participants could pick their top two activities from a predetermined list given by Teaching Lab that aligns with our service offerings and focus. The responses indicated that the top priorities for teachers included supporting diverse student learners (52%), implementing evidence-based instructional practices (42%), and unpacking curriculum (28%) as their highest priority. In addition, nearly 20% of respondents expressed a desire for more PL opportunities that focus on pedagogy from a culturally responsive perspective and on developing effective professional communities with their peers.¬†\n\n\n\nUnderstanding the evolving priorities of teachers at different experience levels is crucial for designing targeted and effective PL opportunities to address educators‚Äô targeted needs and interests throughout their careers.\n\n\n\nDuring the 2022-2023 school year, teachers (76) with less than one year of teaching experience expressed a high level of interest in targeted areas for PL, including:\n\nSupporting diverse learners (i.e., English Learners, students with disabilities, and students with unfinished learning) at almost 60%.\nImplementing effective, evidence-based instructional practices at slightly over 50%.\nUnpacking curriculum at 33%.\nTeaching from a culturally responsive lens at 21%.\nDeveloping effective professional communities with peers at 13%.\n\nTeachers (626) with 1-10 years of teaching experience articulated their desire for PL that supports:\n\nDiverse learners at 54%.\nImplementing effective, evidenced-based instructional practices at 42%.\nUnpacking curriculum at 29%.\nTeaching from a culturally responsive lens at 22%.\nDeveloping effective professional communities with peers at 17%.\n\nIn contrast, educators (546) with 11-20 years of teaching experience articulated their desire for PL in slightly different proportions:\n\nSupporting diverse learners at 50%.\nImplementing effective, evidenced-based instructional practices at 41%.\nUnpacking curriculum at 27%.\nDeveloping effective professional communities with peers at 20%.\nTeaching from a culturally responsive lens at 18%.\n\nIt is important to note that teachers across experience levels also expressed an interest in receiving regular and personalized coaching from instructional experts and becoming facilitators for peer professional learning.\n\n\n\nThere was a 3% distinction between novice and experienced teachers concerning the emphasis on teaching from a culturally responsive lens. As teaching experience increased, this discrepancy expanded to an 8% difference in how novice and experienced teachers prioritized ‚Äúdeveloping effective professional communities with peers.‚Äù This pattern may suggest that novice teachers may place a higher emphasis on cultural responsiveness and collaboration, while more experienced teachers may have already developed proficiency in these areas.¬†\n\n\n\nSupporting diverse learners is a consistent priority for novice and more experienced teachers. This may highlight the ongoing need for PL that addresses the diverse needs of students- including English Learners, students with disabilities, and those with unfinished learning.¬†\n\n\n\nImplementing effective, evidence-based instructional practices is a shared priority among teachers regardless of experience levels. This underscores the importance of a strong pedagogical foundation and the desire for ongoing training in evidence-based teaching methods.¬†\n\n\n\nThe data may indicate a shift in PL priorities based on teaching experience. Teachers with less than one year of experience may prioritize a broad range of topics, strongly emphasizing supporting diverse learners. In contrast, those with 11-20 years of experience may maintain a focus on supporting diverse learners but show a decreased emphasis on unpacking curriculum and teaching from a culturally responsive lens.¬†\n\n\n\nAgain, while unpacking curriculum is a priority for both groups, it is relatively more emphasized by teachers with less than one year of experience. This may suggest a particular interest or perceived need for novice educators to delve into curriculum content and structure.\n\n\n\nIt is important to note that teachers across experience levels also expressed a shared interest in receiving regular and personalized coaching from a content area expert. Additionally, there is interest in ‚Äúbecoming a facilitator for peer professional learning‚Äù‚Äì possibly indicating a desire for active roles in professional development within learning communities.¬†\n\n\n\nThe chart below offers insights into the PL priorities of a diverse sample of teachers- including those who identify as Asian (44), Black/African American (279), Native American or Indian (28), and White (1,185). The six foremost priorities identified were:\n\nSupporting diverse learners (such as English Learners, students with disabilities, and those with unfinished learning).\nImplementing effective, evidence-based instructional practices.\nUnpacking curriculum.\nTeaching from a culturally responsive lens.\nDeveloping effective professional communities with peers.\nBecoming a facilitator for peer professional learning.¬†\n\nThese priorities help provide a snapshot of the shared focus areas among teachers from various racial backgrounds.\n\n\n\n\nThe scores for teaching from a culturally responsive lens indicate a collective challenge across racial groups. Native American or Indian teachers scored notably lower, suggesting potential opportunities for PL in this area. The data highlights the need for increased efforts to integrate cultural responsiveness into teaching practices (Cantrell et. al, 2022).¬†\nHigh-Quality PL May Help Address:¬†\n\nCollaboration & Cultural Exchange\nCurriculum Integration\nEffective Communication¬†\nImplicit Bias Awareness\nInclusive Classrooms¬†\n\n\n\n\nAcross all racial groups, there was a relatively close alignment in scores for evidence-based instructional practices. While there are slight variations, the similarity may suggest a shared commitment to utilizing evidence-based approaches concerning pedagogy‚Äîcutting across racial and cultural backgrounds (Loveless, 2023).¬†¬†\nHigh-Quality PL May Help Enhance Opportunities For:¬†\n\nCoaching\nCollaborative Learning Communities\nEvidence-Based Practices\nFeedback & Reflections¬†\nData-Informed Decision Making\nStandards Alignment\nTechnology Integration\n\n\n\n\nThe scores for developing effective professional communities with peers vary among racial groups. Native American or Indian teachers scored the highest, possibly indicating a strong inclination toward collaborative PL. White teachers scored slightly lower, while Asian and Black teachers scored lower still. These differences may signify varying levels of emphasis or engagement in building professional communities (Simmie, 2023).\nHigh-Quality PL May Help Foster:¬†\n\nCollaborative Planning¬†\nDiversity & Inclusion\nRecognition & Celebrations\nSense of Shared Purpose\nStructured Opportunities for Collaboration\nTeam building¬†\n\n\n\n\nThe difference in scores among racial groups in supporting diverse learners may highlight a potential gap in understanding or addressing the needs of diverse student populations. In this particular sample, Asian teachers scored notably higher than Black and Native American or Indian teachers‚Äîindicating potential differences in educational approaches (Jones et. al, 2019).\nHigh-Quality PL May Help Address:¬†\n\nAccess & Equity\nCultural Competencies¬†\nData-Driven Decision-Making\nDifferentiated Instruction Strategies\nEnglish Learners Needs\nParent & Community Engagement\nSocial-Emotional Learning (SEL) Supports\n\n\n\n\nThe data on becoming a facilitator for peer PL reveals a clear distinction among racial groups. Native American or Indian teachers score the highest, suggesting a potential leadership role in fostering peer learning. Black teachers also scored relatively high, while Asian and White teachers scored lower. This underscores the importance of recognizing and supporting teachers in leadership roles for peer professional development, particularly among Native American or Indian teachers (Canaran, 2023).¬†\nHigh-Quality PL May Help Foster:¬†\n\nContent Knowledge and Expertise\nEducational Best Practices\nInclusive and Safe Spaces for Collaborative Learning\nMentorship & Support Networks\nSupport for Diverse Learning Styles\nUnderstanding Andragogy\n\n\n\n\nThe scores for unpacking curriculum reveal noteworthy differences among teachers of varying racial backgrounds. Black teachers scored higher than others, suggesting a potential emphasis on thorough examination and understanding of curriculum content. Asian and White teachers ranked unpacking curricula slightly lower than their counterparts. But this may be indicative of an area for further exploration (Creech, 2023)\nHigh-Quality PL May Help Address:¬†\n\nAssessment & Evaluation Strategies\nCollaborative Curriculum Planning\nComprehension of Standards & Learning Objectives\nEquity & Inclusion\nFeedback and Reflection Opportunities\nUtilizing Data to Help Inform Instruction\n\n\n\n\nAt the close of SY23, 39% of administrators (55) ranked ‚Äúunpacking curriculum‚Äù as one of their top priorities. These same administrators also deemed ‚Äúimplementing evidence-based practices‚Äù (29%) and ‚Äúdeveloping effective professional communities with peers‚Äù (25%) important. The dataset may have a narrow scope, but it is still important to note that there may be a level of misalignment between the priorities of administrators and teachers. While there is overlap in some areas (i.e., implementing evidence-based instructional practices), the percentages indicate differences in emphasis. Administrators prioritized ‚Äúunpacking curriculum‚Äù and ‚Äúimplementing evidence-based instructional practices‚Äù more than ‚Äúdeveloping effective professional communities with peers‚Äù. On the other hand, teachers prioritized ‚Äúsupporting diverse learners‚Äù more than ‚Äúimplementing evidence-based instructional practices‚Äù and ‚Äúunpacking curriculum.‚Äù\nThis misalignment suggests that administrators may be placing greater emphasis on curriculum planning and instructional methods, while teachers prioritize addressing the needs of diverse learners. To enhance alignment, administrators and teachers may consider continuing to engage in open communication and collaborative decision-making processes to: 1) Help identify shared priorities and 2) Enhance strategies for effectively addressing the needs of both groups.\n\n\nThe majority of Teaching Lab participants (1500) opt for fully In-Person Sessions (28%) or Virtual Sessions (25%); slightly fewer prefer a hybrid model (19%). Below we outline the advantages and considerations of different PL formats.\n\n\n\n\n\n\n\n\nMODALITY\nADVANTAGES\nCONSIDERATIONS\n\n\n\n\nRegular and Personalized Coaching from an Instructional Expert\n-Personalized guidance, immediate feedback, and a tailored approach to individual needs.\n-Deeper and ongoing engagement with professional development.\n-Dedicated time, availability of instructional experts\n-Commitment to regular coaching sessions.\n\n\nVirtual Sessions\n-Flexibility for teachers in geographically diverse locations.\n-Can help accommodate busy schedules, and they may be more cost-effective.\n-Dependence upon technology and reliable internet connections.\n-Potential for reduced interaction compared to in-person sessions\n\n\nIn-Person Sessions:\n-Promotes face-to-face interaction\n-Promotes immediate engagement\n-Fosters a sense of community among participants and can provide a more immersive learning experience.\n-Requires travel and physical presence\n-Involves additional costs for organizing events.\n\n\nHybrid (In-Person and Virtual):\n-Combines the benefits of both in-person and virtual sessions\n-Can accommodate diverse preferences and needs.\n-Requires effective coordination to balance both components.\n-May need to account for technical challenges and potential disparities in participation.\n\n\nAsynchronous Modules (Self-Paced Learning):\n-Provides flexibility for teachers to learn at their own pace and on their own schedule.\n-Accommodates different learning styles and allows for repeated access to materials.\n-Limited immediate interaction and feedback.\n-Some practitioners may prefer more structured and guided learning experiences.\n\n\n\nThese priorities collectively offer a snapshot of the common focus areas among a diverse sample of teachers and administrators‚Äîemphasizing the importance of addressing diverse needs within educational communities. Understanding the evolving priorities of teachers at different experience levels is crucial for designing targeted and effective PL opportunities to address educators‚Äô targeted needs and interests throughout their careers."
  },
  {
    "objectID": "blog/02_beyond_the_basics/index.html#beyond-the-basics-what-teachers-really-seek-from-professional-learning",
    "href": "blog/02_beyond_the_basics/index.html#beyond-the-basics-what-teachers-really-seek-from-professional-learning",
    "title": "Elevating Teacher Growth: Insights into Desired Professional Learning Experiences",
    "section": "",
    "text": "Have you ever attended a professional learning (PL) session and walked away feeling disconnected? Across the country, there needs to be more clarity between what is commonly offered to teachers and what they really want from PL. A school‚Äôs success depends on designing successful, meaningful, innovative PL that kindles teachers‚Äô passion for continuous improvement.¬†\n\nNaturally, teachers want to invest their time and effort in activities that are relevant, practical, and directly applicable to their classrooms. In this blog post, we provide a¬†snapshot of PL opportunities some of our partners are eager to invest their time and effort in according to Teaching Lab survey data.\n\nOver the 2022-2023 school year, 1600 teachers indicated their preferences for upcoming professional learning opportunities. Participants could pick their top two activities from a predetermined list given by Teaching Lab that aligns with our service offerings and focus. The responses indicated that the top priorities for teachers included supporting diverse student learners (52%), implementing evidence-based instructional practices (42%), and unpacking curriculum (28%) as their highest priority. In addition, nearly 20% of respondents expressed a desire for more PL opportunities that focus on pedagogy from a culturally responsive perspective and on developing effective professional communities with their peers.¬†\n\n\n\nUnderstanding the evolving priorities of teachers at different experience levels is crucial for designing targeted and effective PL opportunities to address educators‚Äô targeted needs and interests throughout their careers.\n\n\n\nDuring the 2022-2023 school year, teachers (76) with less than one year of teaching experience expressed a high level of interest in targeted areas for PL, including:\n\nSupporting diverse learners (i.e., English Learners, students with disabilities, and students with unfinished learning) at almost 60%.\nImplementing effective, evidence-based instructional practices at slightly over 50%.\nUnpacking curriculum at 33%.\nTeaching from a culturally responsive lens at 21%.\nDeveloping effective professional communities with peers at 13%.\n\nTeachers (626) with 1-10 years of teaching experience articulated their desire for PL that supports:\n\nDiverse learners at 54%.\nImplementing effective, evidenced-based instructional practices at 42%.\nUnpacking curriculum at 29%.\nTeaching from a culturally responsive lens at 22%.\nDeveloping effective professional communities with peers at 17%.\n\nIn contrast, educators (546) with 11-20 years of teaching experience articulated their desire for PL in slightly different proportions:\n\nSupporting diverse learners at 50%.\nImplementing effective, evidenced-based instructional practices at 41%.\nUnpacking curriculum at 27%.\nDeveloping effective professional communities with peers at 20%.\nTeaching from a culturally responsive lens at 18%.\n\nIt is important to note that teachers across experience levels also expressed an interest in receiving regular and personalized coaching from instructional experts and becoming facilitators for peer professional learning.\n\n\n\nThere was a 3% distinction between novice and experienced teachers concerning the emphasis on teaching from a culturally responsive lens. As teaching experience increased, this discrepancy expanded to an 8% difference in how novice and experienced teachers prioritized ‚Äúdeveloping effective professional communities with peers.‚Äù This pattern may suggest that novice teachers may place a higher emphasis on cultural responsiveness and collaboration, while more experienced teachers may have already developed proficiency in these areas.¬†\n\n\n\nSupporting diverse learners is a consistent priority for novice and more experienced teachers. This may highlight the ongoing need for PL that addresses the diverse needs of students- including English Learners, students with disabilities, and those with unfinished learning.¬†\n\n\n\nImplementing effective, evidence-based instructional practices is a shared priority among teachers regardless of experience levels. This underscores the importance of a strong pedagogical foundation and the desire for ongoing training in evidence-based teaching methods.¬†\n\n\n\nThe data may indicate a shift in PL priorities based on teaching experience. Teachers with less than one year of experience may prioritize a broad range of topics, strongly emphasizing supporting diverse learners. In contrast, those with 11-20 years of experience may maintain a focus on supporting diverse learners but show a decreased emphasis on unpacking curriculum and teaching from a culturally responsive lens.¬†\n\n\n\nAgain, while unpacking curriculum is a priority for both groups, it is relatively more emphasized by teachers with less than one year of experience. This may suggest a particular interest or perceived need for novice educators to delve into curriculum content and structure.\n\n\n\nIt is important to note that teachers across experience levels also expressed a shared interest in receiving regular and personalized coaching from a content area expert. Additionally, there is interest in ‚Äúbecoming a facilitator for peer professional learning‚Äù‚Äì possibly indicating a desire for active roles in professional development within learning communities.¬†\n\n\n\nThe chart below offers insights into the PL priorities of a diverse sample of teachers- including those who identify as Asian (44), Black/African American (279), Native American or Indian (28), and White (1,185). The six foremost priorities identified were:\n\nSupporting diverse learners (such as English Learners, students with disabilities, and those with unfinished learning).\nImplementing effective, evidence-based instructional practices.\nUnpacking curriculum.\nTeaching from a culturally responsive lens.\nDeveloping effective professional communities with peers.\nBecoming a facilitator for peer professional learning.¬†\n\nThese priorities help provide a snapshot of the shared focus areas among teachers from various racial backgrounds.\n\n\n\n\nThe scores for teaching from a culturally responsive lens indicate a collective challenge across racial groups. Native American or Indian teachers scored notably lower, suggesting potential opportunities for PL in this area. The data highlights the need for increased efforts to integrate cultural responsiveness into teaching practices (Cantrell et. al, 2022).¬†\nHigh-Quality PL May Help Address:¬†\n\nCollaboration & Cultural Exchange\nCurriculum Integration\nEffective Communication¬†\nImplicit Bias Awareness\nInclusive Classrooms¬†\n\n\n\n\nAcross all racial groups, there was a relatively close alignment in scores for evidence-based instructional practices. While there are slight variations, the similarity may suggest a shared commitment to utilizing evidence-based approaches concerning pedagogy‚Äîcutting across racial and cultural backgrounds (Loveless, 2023).¬†¬†\nHigh-Quality PL May Help Enhance Opportunities For:¬†\n\nCoaching\nCollaborative Learning Communities\nEvidence-Based Practices\nFeedback & Reflections¬†\nData-Informed Decision Making\nStandards Alignment\nTechnology Integration\n\n\n\n\nThe scores for developing effective professional communities with peers vary among racial groups. Native American or Indian teachers scored the highest, possibly indicating a strong inclination toward collaborative PL. White teachers scored slightly lower, while Asian and Black teachers scored lower still. These differences may signify varying levels of emphasis or engagement in building professional communities (Simmie, 2023).\nHigh-Quality PL May Help Foster:¬†\n\nCollaborative Planning¬†\nDiversity & Inclusion\nRecognition & Celebrations\nSense of Shared Purpose\nStructured Opportunities for Collaboration\nTeam building¬†\n\n\n\n\nThe difference in scores among racial groups in supporting diverse learners may highlight a potential gap in understanding or addressing the needs of diverse student populations. In this particular sample, Asian teachers scored notably higher than Black and Native American or Indian teachers‚Äîindicating potential differences in educational approaches (Jones et. al, 2019).\nHigh-Quality PL May Help Address:¬†\n\nAccess & Equity\nCultural Competencies¬†\nData-Driven Decision-Making\nDifferentiated Instruction Strategies\nEnglish Learners Needs\nParent & Community Engagement\nSocial-Emotional Learning (SEL) Supports\n\n\n\n\nThe data on becoming a facilitator for peer PL reveals a clear distinction among racial groups. Native American or Indian teachers score the highest, suggesting a potential leadership role in fostering peer learning. Black teachers also scored relatively high, while Asian and White teachers scored lower. This underscores the importance of recognizing and supporting teachers in leadership roles for peer professional development, particularly among Native American or Indian teachers (Canaran, 2023).¬†\nHigh-Quality PL May Help Foster:¬†\n\nContent Knowledge and Expertise\nEducational Best Practices\nInclusive and Safe Spaces for Collaborative Learning\nMentorship & Support Networks\nSupport for Diverse Learning Styles\nUnderstanding Andragogy\n\n\n\n\nThe scores for unpacking curriculum reveal noteworthy differences among teachers of varying racial backgrounds. Black teachers scored higher than others, suggesting a potential emphasis on thorough examination and understanding of curriculum content. Asian and White teachers ranked unpacking curricula slightly lower than their counterparts. But this may be indicative of an area for further exploration (Creech, 2023)\nHigh-Quality PL May Help Address:¬†\n\nAssessment & Evaluation Strategies\nCollaborative Curriculum Planning\nComprehension of Standards & Learning Objectives\nEquity & Inclusion\nFeedback and Reflection Opportunities\nUtilizing Data to Help Inform Instruction\n\n\n\n\nAt the close of SY23, 39% of administrators (55) ranked ‚Äúunpacking curriculum‚Äù as one of their top priorities. These same administrators also deemed ‚Äúimplementing evidence-based practices‚Äù (29%) and ‚Äúdeveloping effective professional communities with peers‚Äù (25%) important. The dataset may have a narrow scope, but it is still important to note that there may be a level of misalignment between the priorities of administrators and teachers. While there is overlap in some areas (i.e., implementing evidence-based instructional practices), the percentages indicate differences in emphasis. Administrators prioritized ‚Äúunpacking curriculum‚Äù and ‚Äúimplementing evidence-based instructional practices‚Äù more than ‚Äúdeveloping effective professional communities with peers‚Äù. On the other hand, teachers prioritized ‚Äúsupporting diverse learners‚Äù more than ‚Äúimplementing evidence-based instructional practices‚Äù and ‚Äúunpacking curriculum.‚Äù\nThis misalignment suggests that administrators may be placing greater emphasis on curriculum planning and instructional methods, while teachers prioritize addressing the needs of diverse learners. To enhance alignment, administrators and teachers may consider continuing to engage in open communication and collaborative decision-making processes to: 1) Help identify shared priorities and 2) Enhance strategies for effectively addressing the needs of both groups.\n\n\nThe majority of Teaching Lab participants (1500) opt for fully In-Person Sessions (28%) or Virtual Sessions (25%); slightly fewer prefer a hybrid model (19%). Below we outline the advantages and considerations of different PL formats.\n\n\n\n\n\n\n\n\nMODALITY\nADVANTAGES\nCONSIDERATIONS\n\n\n\n\nRegular and Personalized Coaching from an Instructional Expert\n-Personalized guidance, immediate feedback, and a tailored approach to individual needs.\n-Deeper and ongoing engagement with professional development.\n-Dedicated time, availability of instructional experts\n-Commitment to regular coaching sessions.\n\n\nVirtual Sessions\n-Flexibility for teachers in geographically diverse locations.\n-Can help accommodate busy schedules, and they may be more cost-effective.\n-Dependence upon technology and reliable internet connections.\n-Potential for reduced interaction compared to in-person sessions\n\n\nIn-Person Sessions:\n-Promotes face-to-face interaction\n-Promotes immediate engagement\n-Fosters a sense of community among participants and can provide a more immersive learning experience.\n-Requires travel and physical presence\n-Involves additional costs for organizing events.\n\n\nHybrid (In-Person and Virtual):\n-Combines the benefits of both in-person and virtual sessions\n-Can accommodate diverse preferences and needs.\n-Requires effective coordination to balance both components.\n-May need to account for technical challenges and potential disparities in participation.\n\n\nAsynchronous Modules (Self-Paced Learning):\n-Provides flexibility for teachers to learn at their own pace and on their own schedule.\n-Accommodates different learning styles and allows for repeated access to materials.\n-Limited immediate interaction and feedback.\n-Some practitioners may prefer more structured and guided learning experiences.\n\n\n\nThese priorities collectively offer a snapshot of the common focus areas among a diverse sample of teachers and administrators‚Äîemphasizing the importance of addressing diverse needs within educational communities. Understanding the evolving priorities of teachers at different experience levels is crucial for designing targeted and effective PL opportunities to address educators‚Äô targeted needs and interests throughout their careers."
  },
  {
    "objectID": "blog/06_maintaining_data_quality/index.html",
    "href": "blog/06_maintaining_data_quality/index.html",
    "title": "Data Quality At Teaching Lab: Definitions, Examples, and Best Practices",
    "section": "",
    "text": "Data quality is defined as the degree to which data meets a company‚Äôs expectations of accuracy, validity, completeness, and consistency."
  },
  {
    "objectID": "blog/06_maintaining_data_quality/index.html#why-do-we-care-about-data-quality",
    "href": "blog/06_maintaining_data_quality/index.html#why-do-we-care-about-data-quality",
    "title": "Data Quality At Teaching Lab: Definitions, Examples, and Best Practices",
    "section": "Why Do We Care About Data Quality?",
    "text": "Why Do We Care About Data Quality?\nIt is critical to ensure that the data used for analysis, reporting, and decision-making is reliable and trustworthy. At Teaching Lab, if our data fails to meet our expectations of accuracy, validity, completeness, and consistency, it can have significant negative impacts on our responsive service to partnerships and funders, employee productivity, and key strategies."
  },
  {
    "objectID": "blog/06_maintaining_data_quality/index.html#what-does-data-quality-mean-at-teaching-lab",
    "href": "blog/06_maintaining_data_quality/index.html#what-does-data-quality-mean-at-teaching-lab",
    "title": "Data Quality At Teaching Lab: Definitions, Examples, and Best Practices",
    "section": "What Does Data Quality Mean At Teaching Lab?",
    "text": "What Does Data Quality Mean At Teaching Lab?\nAt Teaching Lab, quality data is key to making accurate, informed decisions regarding the content and focus of our services, dosage and frequency of PL and facilitation, recommendations to external stakeholders, reporting to external funders, etc. The idea of data ‚Äúquality‚Äù can seem ambiguous, however, there are a variety of characteristics that can help us determine what quality data means at Teaching Lab, including:¬†\n\nA majority of Data Quality Assessment Frameworks, including the one used by Teaching Lab‚Äôs Learning & Research team, considers the following 6 elements to be the most important when evaluating the quality of data at any point in time: timeliness, completeness, consistency, uniqueness, integrity, and validity.¬†\nTimeliness: Timeliness refers to how up-to-date data is in collection and reporting. At Teaching Lab, the timeliness of data includes delivery of data for processing (i.e., data collection), and timing (of use) throughout a partnership. For example, baseline data should be collected at baseline and used to inform decisions at the beginning of the year/of a partnership, while end of year data should be collected at end of year and used to report on growth over the course of a partnership and to inform decisions about next steps for a partnership.¬†\nCompleteness: Completeness refers to the percentage of data that is missing within a dataset. Completeness of our data includes the expected vs the actual amount of data that is collected, the expected vs the actual amount of data that is usable for reporting, and the percent completion of individual pieces of data that is collected (e.g., surveys, rubrics, etc.).¬†\nConsistency: Consistency checks that data values in different locations are identical. This applies to naming conventions, contact information, demographics, etc. in data collection, as well as calculated metrics in our reporting. For example, the NPS reported in a Qualtrics dashboard should match the NPS reported in the Ongoing Reports, or teacher information in our Coaching Log should be the same in our Coaching Data Tracker.¬†\nUniqueness: Uniqueness refers to the existence (or lack) of duplicates in our data. Much of our end of year reporting is contingent on a single pre and post instance of data. The existence of duplicate data can skew trends and cause pre-post matching to become exceedingly difficult.¬†\nIntegrity: Integrity refers to how reliable or trustworthy our data is. Integrity looks at the accuracy of our data throughout its lifecycle. The biggest threat to data integrity is user error; this includes mistakes in data entry, accidental deletion or modification of data, and data system misconfigurations.\nValidity: Validity looks at how much our data adheres to our expected data types, formats, value ranges, and internal business rules.\nNote On Data Quality Intersections\nAt Teaching Lab, it‚Äôs important we consider each individual characteristic of data quality, and their intersections. For example, data completeness can impact data‚Äôs timeliness and accuracy. If we have incomplete data by mid-year, we may fail to provide an accurate picture of a partnership thereby impacting the timeliness and usefulness of our insights and recommendations to district leaders at mid-year stepbacks."
  },
  {
    "objectID": "blog/06_maintaining_data_quality/index.html#what-are-some-examples-of-data-quality-red-flags",
    "href": "blog/06_maintaining_data_quality/index.html#what-are-some-examples-of-data-quality-red-flags",
    "title": "Data Quality At Teaching Lab: Definitions, Examples, and Best Practices",
    "section": "What Are Some Examples of Data Quality Red Flags?",
    "text": "What Are Some Examples of Data Quality Red Flags?\nOne of the most crucial pieces of data quality management is the identification and resolution of potential problems. A key tool for identifying these potential data issues at Teaching Lab is our Shared Operations Request form. Following an analysis of the requests submitted throughout SY23-24, the following examples of data quality red flags have been identified:\nüö© Completeness:¬†\n\nAbout 30% of Educator Surveys started in SY23-24 have remained In Progress/Incomplete as of June 2024.\nMissing data at baseline or end of year impacts pre-post comparison n sizes. In order to measure teachers‚Äô growth as well as Teaching Lab‚Äôs impact, data needs to be collected at strategic points in the year (baseline and end of year) for the same teachers.\n\nüö© Consistency:\n\nNon-standardized naming conventions in the classroom observation tool. For example, ‚ÄúSmith‚Äù could be matched to any one of ‚ÄúJ Smith‚Äù, ‚ÄúJohn Smith‚Äù, ‚ÄúCarrie Smith‚Äù, ‚ÄúMs.¬†Smith‚Äù, etc. By first registering teachers we will support with direct-to-teacher coaching in the teacher roster with both their first and last names, we can avoid write-in names in our data instruments.\nParticipants selecting courses in our Participant Feedback survey that are not consistent with their site‚Äôs services.¬†\nRaters submitting classroom observations on different rubrics at the beginning of year and mid or end of year.\n\nüö© Timeliness:\n\nBaseline data collected outside the beginning of year/ beginning of partnership time period does not provide an accurate picture of the starting point of a partnership.¬†\n\nüö© Uniqueness:\n\nParticipants taking our Educator Survey more than once in a given data collection time period.\nRaters submitting multiple observations for a single teacher at baseline, mid, or end of year.\n\nüö© Integrity:\n\nEdits to linked Google Sheets impact the reporting on internal tracking and external facing sheets.¬†\nMistagged services, course names, teacher names, etc. in Qualtrics data.\n\nüö© Validity:\n\nParticipants misread Likert scale questions and select values that do not adhere to statistically expected responses or are not reflective of qualitative responses.¬†\n\nIf data issues, such as duplicate data, missing values, outliers, etc. aren‚Äôt properly addressed, we increase our risk for negative outcomes for the organization and within partnerships."
  },
  {
    "objectID": "blog/06_maintaining_data_quality/index.html#how-does-teaching-lab-ensure-data-quality",
    "href": "blog/06_maintaining_data_quality/index.html#how-does-teaching-lab-ensure-data-quality",
    "title": "Data Quality At Teaching Lab: Definitions, Examples, and Best Practices",
    "section": "How Does Teaching Lab Ensure Data Quality?",
    "text": "How Does Teaching Lab Ensure Data Quality?\nThe Shared Operations Request form helps Learning & Research reactively ensure data quality, however the proactive resolution of potential problems is just as important.¬†\nData Quality in Action: How L&R is Currently Ensuring Data Quality\nListed below are a few of the main steps Learning & Research takes to maintain data quality at Teaching Lab:\n‚úÖ Data ‚ÄúStewards‚Äù - The data analysts on L&R are responsible for cleaning and maintaining the data in each of their assigned data collection instruments.\n‚úÖ Access Controls - Data access controls and user permissions in Qualtrics, Google Suite, and Shiny apps limit who can view and modify data.¬†\n‚úÖ Instrument Revision - At the close of each school year, L&R conducts a construct analysis of applicable surveys using factor analysis, statistical correlations, and internal consistency measures to (1) ensure our tools measure what they are intended to measure, and (2) guide the revision process of making our tools more user-friendly while still providing the necessary data to inform actionable insights.¬†\n‚úÖ Data Collection Plans - L&R uses the collaboratively developed data collection plans to inform the timeliness and completeness of our data.\n‚úÖ Data Trackers - The Ongoing Survey Count and Coaching Data Collection trackers provide ongoing updates on the timeliness and completeness of our data.¬†\n‚úÖ Monday.com Integrations - L&R leverages boards in Monday.com to provide consistent site, course, and teacher names in our data collection and reporting tools, such as the Project Overview Board (based on logistics boards) and direct-to-teacher coaching roster.¬†\n‚úÖ Ongoing Reporting - L&R ensures accessible, ongoing reporting of key metrics through various tools on the DataHub. These tools offer summaries to reflect pre and post breakdowns, granular views of data, and tracking of trends in the data.\nUpcoming: How L&R Will Continue Ensuring Data Quality\nThe insights gained from the Shared Operations Request form, the Programmatic Research & Evaluation Partnership (PREP) Working Group (which fosters collaboration between the Learning and Research team and the Program team to review and refine data collection protocols, instruments, and tools), and L&R Feedback surveys have helped inform next steps for the Learning & Research team. Listed below are key considerations for Learning & Research in the coming school year to continue improving and maintaining data quality at Teaching Lab:¬†\nüîú Data Auditing - In SY24-25, the data analysts will aim to launch a data auditing process (or data ‚Äúhealth check‚Äù reporting) that will monitor the timeliness, completeness, consistency, uniqueness, integrity, and validity of data collected throughout the school year on an ongoing basis.\nüîú Data Literacy Training - Through SY24-25 Learning & Research will be updating/introducing organization wide training to improve data literacy at all levels of Teaching Lab."
  },
  {
    "objectID": "blog/06_maintaining_data_quality/index.html#how-can-i-maintain-teaching-labs-data-quality",
    "href": "blog/06_maintaining_data_quality/index.html#how-can-i-maintain-teaching-labs-data-quality",
    "title": "Data Quality At Teaching Lab: Definitions, Examples, and Best Practices",
    "section": "How Can I Maintain Teaching Lab‚Äôs Data Quality?",
    "text": "How Can I Maintain Teaching Lab‚Äôs Data Quality?\nWhile the Learning & Research team owns data cleaning and reporting, instrument revision, and theory of evaluation, it is everyone‚Äôs responsibility to ensure data quality at Teaching Lab. Here‚Äôs some examples of how you can help:\n\nPrepare For Success: Collaborate on and follow the data collection plans established for your work, complete teacher rosters, and update logistics boards.\nMonitor Your Progress: Reference our data trackers to ensure timely completion of collection throughout the year.\nStay Informed: Familiarize yourself with what data is available and relevant to your work at a given point in time.\nSee Something, Say Something: If you notice an inconsistency in reporting, reach out using the Shared Operations Request form.¬†\nContribute To The Conversation:¬† Consider attending Learning & Research or Data Consultation office hours, partaking in data literacy training initiatives, and joining our PREP working group to continue the cycle of learning and collaboration for you and L&R."
  },
  {
    "objectID": "blog/04_new_mexico_student_survey/index.html",
    "href": "blog/04_new_mexico_student_survey/index.html",
    "title": "Voices from the Classroom: Student Survey Findings from a Rigorous Research Study in New Mexico",
    "section": "",
    "text": "Introduction\nStudent surveys provide invaluable insights directly from Teaching Lab‚Äôs most important stakeholders: students. By tapping into their perceptions, we can gauge the impact of professional learning on teaching practices, especially those aimed at enhancing students‚Äô learning experiences. Through a partnership with the New Mexico Public Education Department (NMPED) and University of Maryland, Teaching Lab has administered student surveys for the past two academic years as part of a rigorous research study, allowing us to capture information about various dimensions of students‚Äô experiences using a unique research design. In this blog post, we will dive deeper into the methodology of the New Mexico study, explore key findings from both the treatment and control groups, and discuss the broader implications of these results.\n\n\nWhat Does Research Tell Us About Student Surveys?\n\n\n\n\n\nThere is an abundance of research that finds (1) teachers impact student perceptions of their classroom environments, and (2) student perceptions of their classroom environments correlate with student learning outcomes. Therefore, we have included student surveys on classroom learning environments as part of Teaching Lab‚Äôs research agenda and theory of evaluation.¬†\nWe can rely on students‚Äô perceptions of teachers because students know good teaching when they see it: The Measures of Effective Teaching (MET Project, 2010) project found that students perceived clear differences among teachers. Student feedback about teachers is more consistent than that provided by administrators after classroom observations or those based solely on student test scores.¬†\nIt is important to understand student perceptions of teacher quality because this perception is correlated to students‚Äô learning outcomes: Hattie (2017)‚Äôs meta-analysis of 1,400 studies found that overall, student ratings of teaching quality has a 0.50 effect size, which means that students‚Äô evaluations of their teachers are significantly correlated with their academic achievement. Additionally, the MET Project (2012) found that students in classes where students perceived their teachers to be in the top quartile of effectiveness learned 4.8 months of schooling more in math compared to students in classes where students perceived their teachers to be in the bottom quartile of effectiveness.¬†\nStudent surveys can improve classroom environments: Potvin (2021) conducted a study to evaluate the impact of student surveys on classroom environments and teachers‚Äô pedagogical practices. Results indicated that regular discussion of student survey results encouraged teachers to adapt their practices, assess their classroom environments formatively, and contribute to a teacher learning environment.¬†\nTeachers have control over many aspects of students‚Äô behaviors that impact their learning: Jackson (2019) found that teachers influence students‚Äô attendance, behavior, and engagement, dimensions of their learning experiences which are highly predictive of high school graduation and college outcomes. Jackson (2019) estimated that the impact of teachers on behavior is about 10 times more predictive of whether they increase students‚Äô high-school completion than their impacts on test scores. Specifically, the following are within the teacher‚Äôs control and have documented impacts on student learning.\n\n\n\n\n\n\nTeacher-Student Relationships: Often perceived as the softer aspect of education, teacher-student relationships are actually crucial to the success of especially underserved students (Wacker and Olson, 2019). These relationships significantly impact academic achievement, with Riorda et al.‚Äôs 2011 meta-analysis showing an effect size of 0.16. Hattie (2017) suggests they are more influential than programs like Reading Recovery and other instructional strategies.\nSelf-Efficacy: Self-efficacy, or the belief in one‚Äôs ability to achieve a goal, greatly influences student engagement and resilience (Blazar & Kraft, 2017; Transforming Education, 2019). Hattie (2017) also notes its strong potential to boost academic performance.\nBeing Challenged: Teachers are key to ensuring students are consistently academically challenged. Significant correlations were found in the Tripod student survey between teacher effectiveness and students learning a lot almost every day (œÅ=0.273) and learning to correct mistakes (œÅ=0.264) (MET Project, 2010).\nHappiness and Sense of Belonging: Teachers play a pivotal role in influencing students‚Äô happiness, which in turn affects academic performance (Blazar & Kraft, 2017; Quinn & Duckworth, 2006).\n\nCulturally Responsive and Sustaining Education (CRSE): Combining rigorous academic content with culturally responsive practices significantly enhances student outcomes (Gay, 2018; Hammond, 2014; Ladson-Billings, 2021). Thorough research, including experimental and advanced statistical models, confirms the impact of culturally responsive teaching on academic achievement (Bradshaw et al., 2018; Byrd, 2016; Dee & Penner, 2017; Kelley et al.¬†2015; Portes et al.¬†2017).\n\n\nBackground and Methodology for New Mexico Student Surveys\nDuring SY22-23 and SY23-24, Teaching Lab engaged in an external research project with the University of Maryland to examine the impact of our Professional Learning on secondary mathematics educators across New Mexico. In partnership with NMPED staff at the Math and Science Bureau, Teaching Lab provided professional learning (PL) aimed at exploring teacher and student math identities to 19 teachers from six secondary schools in SY22-23 to 22 teachers from 12 schools in SY23-24.\nUsing a staggered cohort design, half of the participants were randomly assigned to participate in PL in the fall semester (treatment group) and the other half to take part in the spring (control group). Teachers were randomly assigned within school ‚Äúblocks‚Äù to ensure that, for each school, half of the teachers participated in PL in the fall and the other half participated in the spring. We compared outcomes between the treatment and control groups at the end of the fall semesters, when the treatment group had completed one semester of PL, and the control group had not yet started their PL. Included in the data collection was a student survey that measured their beliefs about how their teacher engaged in culturally responsive and sustaining education (CRSE) practices and built meaningful relationships with students, as well as how students rated their own self-efficacy, happiness, and feeling of being challenged in their math class.\n\n\nPreliminary Descriptive Results\nIn total 461 New Mexico students completed Teaching Lab‚Äôs PL student survey across SY2022-2023 and SY2023-2024. Of these students, 282 were taught by teachers who had already participated in PL, while 179 students were taught by teachers who had not yet participated in PL at the time of the survey administration.\n\n\nStudent Demographics\nOf the 461 students who were surveyed, approximately 58% identified as non-white or more than one race. Additionally, approximately 49% of students were female and 67% identified as Hispanic. There were some imbalances between the intervention and control group, specifically for students identifying as White, more than one race, and other.\n\n\n\nOverall Results\nPreliminary findings based on descriptive analyses indicate promising impacts on students‚Äô educational experiences in math classrooms. In particular, students whose teachers had already participated in the intervention reported more positive perceptions of their teachers‚Äô use of culturally responsive teaching methods, in addition to greater self-confidence in their mathematical skills.\n\n\n\n\n\n\n\nCRSE\nStudents whose teachers participated in the professional learning intervention noted a greater frequency of culturally responsive and sustaining educational practices compared to their counterparts in the control group. This observation is particularly favorable considering the intervention‚Äôs emphasis on nurturing students‚Äô mathematical identities by applying CRSE methodologies. Such feedback from students underscores the promising effects of the PL on improving teachers‚Äô CRSE awareness and related instructional practices.¬†\n\n\n\nNote: % positive indicators refers to the percentage of students that responded ‚Äúoften‚Äù or ‚Äúalways‚Äù on a five-point Likert scale. The overall score is a composite measure derived from the individual items.\n\n\n\n\nStudent Self-Efficacy\nAlongside CRSE practices, students of teachers in the intervention group exhibited higher self-efficacy in math learning than students of teachers in the control group. This finding suggests that the intervention‚Äôs focus on promoting students‚Äô unique math identities not only enhances instructional methods, but also cultivates students‚Äô self-confidence in their learning abilities.\n\n\n\nNote: % positive indicators refers to the percentage of students that responded ‚Äúsomewhat agree‚Äù or ‚Äúagree‚Äù on a five-point Likert scale. The overall score is a composite measure derived from the individual items.\n\n\n\n\nFuture Implications\nAcross both academic years, student-reported CRSE practices have been consistently higher for teachers in the treatment group compared to teachers in the control group, indicating that Teaching Lab‚Äôs PL has been effective at improving teachers‚Äô CRSE practices. Moreover, the reported increase in students‚Äô self-efficacy related to their mathematical skills highlights a shift towards a more holistic approach to instruction, where fostering positive identities and growth mindsets for students becomes as pivotal as mastering the subject matter itself.\nThese results suggest a promising avenue for future educational strategies, where targeted PL initiatives could serve as a key lever in advancing equity and inclusion within classrooms. Such an approach could have far-reaching implications beyond math classrooms, suggesting a roadmap for enhancing student outcomes across a wide range of academic disciplines.\n\n\nWhat‚Äôs Next for Student Surveys at Teaching Lab?\nThe New Mexico research study has revealed promising results about Teaching Lab‚Äôs impact on not only teachers, but also students‚Äô self-efficacy and self-reported CRSE practices. Our external evaluator, Dr.¬†David Blazer from the University of Maryland, and his team are in the process of performing rigorous analysis and drafting a research report to detail the findings, which the L&R team will share in the coming months. These data insights are invaluable for funders, external partners, and internal team members alike. Although it may require some legwork to secure district approval, we encourage more project teams to incorporate student surveys into their data collection plan.¬†\nIf you are interested in checking out our student survey results from other sites (i.e.¬†EIC-Rochester, D11, or Arkansas), you can head to Section 7 of their respective Ongoing Reports.\n\n\n\nNote: % positive indicators refers to the percentage of students that responded ‚Äúoften‚Äù or ‚Äúalways‚Äù on a five-point Likert scale. The overall score is a composite measure derived from the individual items.\nNote: % positive indicators refers to the percentage of students that responded ‚Äúsomewhat agree‚Äù or ‚Äúagree‚Äù on a five-point Likert scale. The overall score is a composite measure derived from the individual items."
  },
  {
    "objectID": "index.html#faq",
    "href": "index.html#faq",
    "title": "Data Hub",
    "section": "FAQ",
    "text": "FAQ\n\nHow do I navigate the data hub?\n\nCheck out this Zoom Clip (to be added soon) for a full breakdown! Generally the content is organized by the navigation bar above, but also feel free to use the search bar. The key pages to know about are dashboards, trackers, and reports, which are all listed in the navigation bar.\n\nHow do I know what dashboard I want?\n\nThat depends on what kind of data you are looking for, one likely answer is the ongoing report since it has a broad overview of all L&R data, but there are more detailed, custom dashboards for other purposes organized by the Guskey Framework which drives all of our research, on the ‚ÄúDashboards‚Äù Page, with detailed descriptions and tutorials describing their individual use cases.\n\nWhat is on here?\n\nHere‚Äôs a short list: L&R Dashboards, Reports, and Trackers. Additionally our blog, data dives, and much more L&R informative content under the resources page - check that out!\n\nHow often does the data update?\n\nIt depends! Generally speaking with the frequency necessary for each data tools business case. For more details read here.\n\nWhere is data from previous years?\n\nCheck out the archives tab!"
  },
  {
    "objectID": "index.html#pages",
    "href": "index.html#pages",
    "title": "Data Hub",
    "section": "Pages",
    "text": "Pages\n\n\n\n\n\n\n\n\n\n\nReports\n\n\nReports from previous years, and this year‚Äôs ongoing report which provides continuous updated data by site.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDashboards\n\n\nDashboards are available to review data frome each individual survey instrument and part of the Guskey Framework. They are developed and maintained by Duncan Gates, please‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeaching Lab: Professional Learning Evaluation Plan\n\n\nA background on who, why, and what we measure with links to specific survey instruments\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog\n\n\nVarious musings and writings on the data that Teaching Lab collects, and how it is used to conduct research and improve student and teacher outcomes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Dives\n\n\nThe Learning & Research team‚Äôs series on how, what, why and where we use data - with video recordings and slides linked.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrganizational Goal Tracking\n\n\nCheck in on how Teaching Lab is doing at achieving it‚Äôs organizational goals - animated by quarter with the ability to view each quarter individually\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the Data\n\n\nLearn how often the data updates, where it comes from, and how items like student work, course assessments, or mindsets are scored.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoom Channel\n\n\nThis channel tutorials for dashboards and reports and the learning evaluation plan.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Data Hub",
    "section": "Contact",
    "text": "Contact\nDuncan Gates\n\nStaff Data Scientist\nLearning & Research Staff Page\nSlack\nemail: duncan.gates@teachinglab.org\nCurrent Time Zone: MST"
  },
  {
    "objectID": "trademark.html",
    "href": "trademark.html",
    "title": "Trademark Policy",
    "section": "",
    "text": "This policy is adapted directly from the WordPress Foundation‚Äôs trademark policy for the WordPress and WordCamp names and logos. We admire the job that WordPress has done building a thriving open source community while at the same time making possible a wide variety of WordPress related businesses. We hope that this policy will help us do the same for Quarto."
  },
  {
    "objectID": "trademark.html#goals",
    "href": "trademark.html#goals",
    "title": "Trademark Policy",
    "section": "Goals",
    "text": "Goals\n\nTBD"
  },
  {
    "objectID": "archives.html",
    "href": "archives.html",
    "title": "Data Archives",
    "section": "",
    "text": "Ongoing Report SY23-24\nFY24 Coaching Tracker"
  },
  {
    "objectID": "dashboard4/participant_practice_2.html",
    "href": "dashboard4/participant_practice_2.html",
    "title": "IPG Ongoing Report Data",
    "section": "",
    "text": "Go to section 4 of the ongoing report ‚û°Ô∏è"
  },
  {
    "objectID": "dashboard4/participant_practice_4.html",
    "href": "dashboard4/participant_practice_4.html",
    "title": "IPG Qualtrics Dashboard",
    "section": "",
    "text": "Go to the qualtrics dashboard ‚û°\n\n&lt;iframe src=‚Äúhttps://docs.google.com/document/d/e/2PACX-1vRTxmNmVV9dR6jguGB8M3GFn8w1xGj8YFMkPkj52venVC9qRCQ14zqBJHt37ggoptUKpYIvFEG0JI3D/pub?embedded=true‚Äù style=‚Äúwidth: 1000px; border: none; margin: 0; height: 800px;‚Äù"
  },
  {
    "objectID": "dashboard3/course_assessments.html",
    "href": "dashboard3/course_assessments.html",
    "title": "Course Assessments Dashboard",
    "section": "",
    "text": "Go to the course assessments dashboard ‚û°Ô∏è"
  },
  {
    "objectID": "dashboard2/educator_survey.html",
    "href": "dashboard2/educator_survey.html",
    "title": "Educator Survey // Ongoing Report",
    "section": "",
    "text": "Go to section 3 of the ongoing report ‚û°Ô∏è"
  },
  {
    "objectID": "dashboard5/student_learning_experiences_1.html",
    "href": "dashboard5/student_learning_experiences_1.html",
    "title": "Student Outcomes // Ongoing Report",
    "section": "",
    "text": "Go to section 5 of the ongoing report ‚û°Ô∏è"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "evaluation_plan.html",
    "href": "evaluation_plan.html",
    "title": "Teaching Lab: Professional Learning Evaluation Plan",
    "section": "",
    "text": "Background\nTeaching Lab‚Äôs mission is to fundamentally shift the paradigm of teacher professional learning (PL) to achieve educational equity. In our fourth full-year of implementation of professional learning with school partners, Teaching Lab seeks to answer the following questions about the impact of our work:¬†\n\nTo what extent do teachers experience and sustain a change in equitable mindsets, content and pedagogical content knowledge, and equitable teaching practices after participating in Teaching Lab PL?\nTo what extent do the learning experiences and academic outcomes of students of teachers who participate in Teaching Lab PL experience improve?\n\nOur theory of evaluation for our professional learning model is adapted from the Guskey (2009) framework: If teachers find their professional learning experience valuable (participant perceptions), if they increase content and pedagogical content knowledge (participant knowledge), if they have an equitable mindset and high expectations for all students (participant mindsets), if they have the conducive environment for professional learning (enabling conditions), and if they enact evidence-based instructional practices in the classroom (participant practices), then their students will have improved learning outcomes (student learning experiences).\nTeaching Lab has designed a rigorous evaluation plan of our professional learning, with multiple venues for collecting data about each component within our theory of evaluation. We have obtained agreements from almost every single one of our system partners to engage in data collection, and we are excited to triangulate various data points to tell the story of where Teaching Lab is changing teaching and learning, and where we can adapt our model to reach even better results.¬†\nIn addition to our internal evaluation plan, Teaching Lab is engaged in external evaluation studies led by prominent education researchers, Dr.¬†Heather Hill, Dr.¬†John Papay, and Dr.¬†David Blazar, as well as the The Bill & Melinda Gates Foundation. These studies will¬† measure changes in teacher mindsets, knowledge, and practices in addition to¬† student learning experiences and outcomes, both academic and social emotional, to better understand the impact ofTeaching Lab professional learning.\n\n\nWhat We Measure\nThe following describes how we plan to learn about each component of our theory of evaluation:¬†\n\nParticipant perceptions:\n\nWe administer an End-of-Session Survey which allows us to learn about participants‚Äô feedback of the facilitation and the overall quality and content of the sessions. We use this information to make real-time adjustments to facilitation.\nWe also administer an End-of-Course Survey, which allows us to learn whether participants believe the PL is high quality, relevant, and feasible, and whether they would recommend it to others (Net Promoter Score). We use this information to make any necessary changes to the course structure and/or content focus.\n\nParticipant knowledge:¬†\n\nDuring the first session, participants complete a short knowledge assessmentconsisting of multiple-choice quiz questions about the specific content and pedagogical content knowledge addressed in the sequence of learning for a content area. We use this information to adjust facilitation as needed. During the last session, they complete the same set of multiple-choice quiz questions. We use this information to track how participant knowledge changes.\n\nParticipant mindsets:¬†\n\nWe conducted a literature review to identify key teacher mindsets and expectations which are most predictive of student learning outcomes, particularly the outcomes of students of color: recognition of race and culture, high expectations that all students can and will learn, and growth mindsets. Moreover, we have incorporated items on self-efficacy to deliver culturally responsive instruction, which is linked to high expectations and effective teaching practices. We‚Äôve adapted the questions on teacher mindsets to be relevant for coaches and school leaders. We continue to integrate these mindsets into our PL sequence.\nWe ask about these characteristics in our Diagnostic Educator Survey and Follow-up Educator Survey. We use this information to track how participant mindsets change throughout our partnership.¬†\nThe mindset questions can be found in the Mindsets and Expectations section of our Educator Survey.\nWe also triangulate these survey responses with students‚Äô perceptions of their learning experience. Read more about our work on the student perception survey in the ‚ÄúStudent learning experiences‚Äù section below.\n\nSupportive structures and environment: Teaching Lab‚Äôs grassroots model of professional learning leans on the core belief that teachers deserve to feel motivated and supported by their peers to learn and grow. In teacher-led communities, educators are more likely to buy into their own development and work collaboratively with their colleagues to improve instruction. We call this the ‚Äúheart‚Äù of professional learning.¬†\n\nIn our Diagnostic Educator Survey and Follow-up Educator Survey, we ask teachers about their trust of other teachers, and the level of collaboration they have with others in their schools, which are parts of teacher social capital. We use this information to track how teacher social capital changes throughout our partnership.\nThe supportive structures and environment questions can be found in the School Environment section of our Diagnostic Educator Survey and Follow-up Educator Survey.\n\nParticipant practices:¬†\n\nOur team conducts classroom observations and takes the opportunity to capture standards-and-shifts-aligned instruction using a subset of core actions from the Instructional Practice Guides which are most emphasized in our learning sequence (e.g., citing relevant evidence, productive struggle, student talk) throughout our partnership.¬†\nWe also sign data agreements with state education agencies and districts when possible to obtain teacher evaluation data to compare the effectiveness of teachers participating in our professional learning sequences compared to other teachers.¬†\n\nStudent learning experiences: Our ultimate goal is to increase student learning and achieve educational equity.¬†\n\nWe conducted a literature review to identify key dimensions of student learning experiences that are within teachers‚Äô control and highly predictive of student learning outcomes: student-teacher relationships, self-efficacy, growth mindset, happiness and sense of belonging, being challenged and culturally responsive teaching practices. We ask about these dimensions in our twice-a-year student survey based on Transcend‚Äôs Leaps Student Voice Survey.\nWe also collect and analyze student tasks from the same students multiple times throughout the school year to track the quality of student tasks and student performance on rigorous, grade-level tasks throughout our partnership.\nWe sign data agreements with state education agencies and districts when possible to obtain student formative and/or summative assessment data to track learning."
  },
  {
    "objectID": "On_the_data.html",
    "href": "On_the_data.html",
    "title": "On the Data",
    "section": "",
    "text": "This page is currently under maintenance, check back later for more information!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dashboard5/student_learning_experiences_3.html",
    "href": "dashboard5/student_learning_experiences_3.html",
    "title": "Student Work Graded Dashboard",
    "section": "",
    "text": "Go to the student work grades dashboard ‚û°Ô∏è\n\nSee the Loom Tutorial Here"
  },
  {
    "objectID": "dashboard5/student_learning_experiences_4.html",
    "href": "dashboard5/student_learning_experiences_4.html",
    "title": "Student Work Grading Web App",
    "section": "",
    "text": "Go to the student work grading app ‚û°Ô∏è"
  },
  {
    "objectID": "dashboard2/educator_survey_2.html",
    "href": "dashboard2/educator_survey_2.html",
    "title": "Educator Survey Qualtrics Data",
    "section": "",
    "text": "Go to the qualtrics dashboard ‚û°\n\n&lt;iframe src=‚Äúhttps://docs.google.com/document/d/e/2PACX-1vRTxmNmVV9dR6jguGB8M3GFn8w1xGj8YFMkPkj52venVC9qRCQ14zqBJHt37ggoptUKpYIvFEG0JI3D/pub?embedded=true‚Äù style=‚Äúwidth: 1000px; border: none; margin: 0; height: 800px;‚Äù"
  },
  {
    "objectID": "dashboard3/course_assessments_2.html",
    "href": "dashboard3/course_assessments_2.html",
    "title": "Course Assessments // Ongoing Report",
    "section": "",
    "text": "Go to section 2 of the Ongoing Report ‚û°Ô∏è"
  },
  {
    "objectID": "dashboard4/participant_practice_3.html",
    "href": "dashboard4/participant_practice_3.html",
    "title": "Google Sheets Raw Copy of Classroom Observations",
    "section": "",
    "text": "Go to the by coach google sheets\nGo to the by site google sheets\nGo to the by rubric google sheet"
  },
  {
    "objectID": "dashboard4/participant_practice_1.html",
    "href": "dashboard4/participant_practice_1.html",
    "title": "IPG Raw Data/Downloadable Dashboard",
    "section": "",
    "text": "Go to the IPG Raw/Downloadable Dashboard ‚û°Ô∏è"
  },
  {
    "objectID": "Reports.html",
    "href": "Reports.html",
    "title": "Reports",
    "section": "",
    "text": "Reports on Teaching Lab Data"
  },
  {
    "objectID": "Reports.html#ongoing-report",
    "href": "Reports.html#ongoing-report",
    "title": "Reports",
    "section": "Ongoing Report",
    "text": "Ongoing Report\nA dashboard to comprehensively provide an overview of Teaching Lab‚Äôs L&R data, as well as handy site-specific summaries."
  },
  {
    "objectID": "Reports.html#ongoing-report-1",
    "href": "Reports.html#ongoing-report-1",
    "title": "Reports",
    "section": "Ongoing Report",
    "text": "Ongoing Report\nA fully compiled, updating, interactive AI-utilizing report for all L&R data representing each part of the Guskey Framework. Sections do not correspond exactly to the guskey framework but are enumerated on the context page. Additionally includes a PDS page in the last section where all PDS data can be accessed."
  },
  {
    "objectID": "Reports.html#cps-skyline-pl-participant-feedback",
    "href": "Reports.html#cps-skyline-pl-participant-feedback",
    "title": "Reports",
    "section": "CPS Skyline PL Participant Feedback",
    "text": "CPS Skyline PL Participant Feedback\nA custom built CPS Skyline Report on Participant Feedback"
  },
  {
    "objectID": "Reports.html#cleveland-sy23-24-lbtt",
    "href": "Reports.html#cleveland-sy23-24-lbtt",
    "title": "Reports",
    "section": "Cleveland SY23-24 LBTT",
    "text": "Cleveland SY23-24 LBTT\nA custom built report on trends, environment, planning, and writing for Cleveland LBTT"
  },
  {
    "objectID": "Reports.html#previous-years-reports-static",
    "href": "Reports.html#previous-years-reports-static",
    "title": "Reports",
    "section": "Previous Year‚Äôs Reports (Static)",
    "text": "Previous Year‚Äôs Reports (Static)\nR-studio project template and workflow for sharing psychological research projects in the form of a website."
  },
  {
    "objectID": "Reports.html#sy21-22-reports-static",
    "href": "Reports.html#sy21-22-reports-static",
    "title": "Reports",
    "section": "SY21-22 Reports (Static)",
    "text": "SY21-22 Reports (Static)\nR-studio project template and workflow for sharing psychological research projects in the form of a website."
  },
  {
    "objectID": "Reports.html#sy20-21-reports-static",
    "href": "Reports.html#sy20-21-reports-static",
    "title": "Reports",
    "section": "SY20-21 Reports (Static)",
    "text": "SY20-21 Reports (Static)\nR-studio project template and workflow for sharing psychological research projects in the form of a website."
  },
  {
    "objectID": "blog/01_changing_the_game/index.html",
    "href": "blog/01_changing_the_game/index.html",
    "title": "Changing the Game: How High-Dosage Coaching Shifts Teacher Practices & Student Success",
    "section": "",
    "text": "Over the past two years, Teaching Lab‚Äôs direct-to-teacher coaching model has seen remarkable growth. Beginning in the spring of 2021 with 90 teachers across the state of Mississippi, Teaching Lab scaled in SY22-23 to provide coaching services to more than 300 teachers in New York City, Chicago, and Arkansas. So far in SY23-24, Teaching Lab has over 800 teachers registered to participate in direct-to-teacher coaching.\nThrough analysis of classroom observations, student work samples, and formative and summative assessment data, the impact Teaching Lab‚Äôs coaching has on teachers and students through our coaching model is clear: teachers improve their practices and students improve their academic performance.\n\n\n\nCoaching is one of the most promising professional learning interventions. The results of a meta-analysis from 60 studies conducted by Kraft et al., (2018) indicate that, on average, teacher coaching programs were associated with a substantial improvement in instructional practice, with an effect size of 0.49 standard deviations. This is like turning a good teacher into a really great teacher. Additionally, student academic achievement received a boost, showing an effect size of 0.18 standard deviations. While the effect is smaller, some researchers, such as Kraft (2020) again, consider 0.20 standard deviations to be a large effect size for causal studies of pre-K‚Äì12 education interventions evaluating effects on student achievement ‚Äì so this is very meaningful progress.\n\n\n\nTeaching Lab conducted over 600 classroom observations for our direct-to-teacher partnerships in SY22-23. On average, teachers increased the percentage of positive indicators on the ELA and Math Instructional Practice Guides and Foundational Skills Observational Tool by 33 percentage points, from 46% at baseline to 79% at the end of the year. (Note: Scoring 3 or 4 on a 4-point Likert Scale and Yes on Yes/No items on the observational tools are considered positive indicators.)¬†\n\n\n\n\nTeaching Lab also analyzed over 2000+ work samples from our direct-to-teacher coaching partnerships in SY22-23. Overall, we saw a 14 percentage point increase in the percentage of students demonstrating proficiency on grade-level tasks for these sites, from 30% to 44% at the beginning and end of our time working with teachers. (Note: scoring a 2 on the 2-point student sample scoring rubrics is considered to demonstrate proficiency.)\n\n\n\nEach ‚Äústudent‚Äù represents 1%, looking at the students you can see more work got a 2 on the 2nd iteration of student work grading, and fewer received 0‚Äôs.\n\n\nWe saw the most growth in Chicago Network 7 and New York City District 9.\n\n\n\n\nFinally, for one of our district partners, we were able to conduct the first-ever classroom-level analysis of student assessment data in which we compared student assessment scores from classrooms Teaching Lab coached to scores from classrooms we did not. Results demonstrated that classrooms of teachers coached by Teaching Lab outperformed classrooms not coached by Teaching Lab at the same schools on early literacy assessments. Specifically, Teaching Lab-coached classrooms had a higher average percentage of students at/above benchmark in the spring (29 percentage point difference) and demonstrating typical/better progress from fall to spring (18 percentage point difference) compared to non-Teaching Lab classrooms.\nWhat‚Äôs next for understanding the impact of Teaching Lab‚Äôs coaching model?\nAs we grow and scale our model, data collection and analysis is becoming more robust and richer. By utilizing data collected from the coaching log, the Learning & Research Team will explore correlations between the frequency and duration of coaching sessions, the specific areas of focus and goals within coaching cycles, and the resulting outcomes at the end of SY23-24. We will also participate in an exploratory RPPL study to better understand the ‚Äúmoves‚Äù our coaches make with teachers and how they could be leveraged in the development of an AI feedback tool. Finally, through two federal grants that Teaching Lab has won, the Education Innovation Research mid-phase grant in partnership with ASSISTments and the Teacher and School Leader Incentive grant, we will engage in randomized control trials that will rigorously evaluate the effectiveness of our coaching model.\nFor more information on Teaching Lab‚Äôs impact, see here.\n\n\n\nEach ‚Äústudent‚Äù represents 1%, looking at the students you can see more work got a 2 on the 2nd iteration of student work grading, and fewer received 0‚Äôs."
  },
  {
    "objectID": "blog/01_changing_the_game/index.html#changing-the-game-how-high-dosage-coaching-shifts-teacher-practices-student-success",
    "href": "blog/01_changing_the_game/index.html#changing-the-game-how-high-dosage-coaching-shifts-teacher-practices-student-success",
    "title": "Changing the Game: How High-Dosage Coaching Shifts Teacher Practices & Student Success",
    "section": "",
    "text": "Over the past two years, Teaching Lab‚Äôs direct-to-teacher coaching model has seen remarkable growth. Beginning in the spring of 2021 with 90 teachers across the state of Mississippi, Teaching Lab scaled in SY22-23 to provide coaching services to more than 300 teachers in New York City, Chicago, and Arkansas. So far in SY23-24, Teaching Lab has over 800 teachers registered to participate in direct-to-teacher coaching.\nThrough analysis of classroom observations, student work samples, and formative and summative assessment data, the impact Teaching Lab‚Äôs coaching has on teachers and students through our coaching model is clear: teachers improve their practices and students improve their academic performance.\n\n\n\nCoaching is one of the most promising professional learning interventions. The results of a meta-analysis from 60 studies conducted by Kraft et al., (2018) indicate that, on average, teacher coaching programs were associated with a substantial improvement in instructional practice, with an effect size of 0.49 standard deviations. This is like turning a good teacher into a really great teacher. Additionally, student academic achievement received a boost, showing an effect size of 0.18 standard deviations. While the effect is smaller, some researchers, such as Kraft (2020) again, consider 0.20 standard deviations to be a large effect size for causal studies of pre-K‚Äì12 education interventions evaluating effects on student achievement ‚Äì so this is very meaningful progress.\n\n\n\nTeaching Lab conducted over 600 classroom observations for our direct-to-teacher partnerships in SY22-23. On average, teachers increased the percentage of positive indicators on the ELA and Math Instructional Practice Guides and Foundational Skills Observational Tool by 33 percentage points, from 46% at baseline to 79% at the end of the year. (Note: Scoring 3 or 4 on a 4-point Likert Scale and Yes on Yes/No items on the observational tools are considered positive indicators.)¬†\n\n\n\n\nTeaching Lab also analyzed over 2000+ work samples from our direct-to-teacher coaching partnerships in SY22-23. Overall, we saw a 14 percentage point increase in the percentage of students demonstrating proficiency on grade-level tasks for these sites, from 30% to 44% at the beginning and end of our time working with teachers. (Note: scoring a 2 on the 2-point student sample scoring rubrics is considered to demonstrate proficiency.)\n\n\n\nEach ‚Äústudent‚Äù represents 1%, looking at the students you can see more work got a 2 on the 2nd iteration of student work grading, and fewer received 0‚Äôs.\n\n\nWe saw the most growth in Chicago Network 7 and New York City District 9.\n\n\n\n\nFinally, for one of our district partners, we were able to conduct the first-ever classroom-level analysis of student assessment data in which we compared student assessment scores from classrooms Teaching Lab coached to scores from classrooms we did not. Results demonstrated that classrooms of teachers coached by Teaching Lab outperformed classrooms not coached by Teaching Lab at the same schools on early literacy assessments. Specifically, Teaching Lab-coached classrooms had a higher average percentage of students at/above benchmark in the spring (29 percentage point difference) and demonstrating typical/better progress from fall to spring (18 percentage point difference) compared to non-Teaching Lab classrooms.\nWhat‚Äôs next for understanding the impact of Teaching Lab‚Äôs coaching model?\nAs we grow and scale our model, data collection and analysis is becoming more robust and richer. By utilizing data collected from the coaching log, the Learning & Research Team will explore correlations between the frequency and duration of coaching sessions, the specific areas of focus and goals within coaching cycles, and the resulting outcomes at the end of SY23-24. We will also participate in an exploratory RPPL study to better understand the ‚Äúmoves‚Äù our coaches make with teachers and how they could be leveraged in the development of an AI feedback tool. Finally, through two federal grants that Teaching Lab has won, the Education Innovation Research mid-phase grant in partnership with ASSISTments and the Teacher and School Leader Incentive grant, we will engage in randomized control trials that will rigorously evaluate the effectiveness of our coaching model.\nFor more information on Teaching Lab‚Äôs impact, see here.\n\n\n\nEach ‚Äústudent‚Äù represents 1%, looking at the students you can see more work got a 2 on the 2nd iteration of student work grading, and fewer received 0‚Äôs."
  },
  {
    "objectID": "blog/05_advancing_data_equity/index.html#background",
    "href": "blog/05_advancing_data_equity/index.html#background",
    "title": "Advancing Data Equity: Progress and Future Strategies",
    "section": "Background",
    "text": "Background\nProgram evaluation and research are not objective; at minimum, inherent biases can affect research design, execution, or results. At Teaching Lab, it is our responsibility to implement equitable practices into our research with the ultimate goal of minimizing bias. In alignment with Teaching Lab‚Äôs mission to advance educational equity, the Learning & Research (L&R) Team has designed a comprehensive Data Equity Plan, implementing several key initiatives in FY24 that will continue into FY25. The purpose of this plan is threefold: define action steps that the L&R Team may take to mitigate bias in our evaluation, center the lived experiences of the educators we serve, and promote our organization‚Äôs pursuit of educational equity."
  },
  {
    "objectID": "blog/05_advancing_data_equity/index.html#what-is-data-equity",
    "href": "blog/05_advancing_data_equity/index.html#what-is-data-equity",
    "title": "Advancing Data Equity: Progress and Future Strategies",
    "section": "What is Data Equity?",
    "text": "What is Data Equity?\nData equity refers to the intentional and transparent process of making decisions in data projects to ensure fair representation and outcomes. It acknowledges that while data science tools are unbiased, the human decisions involved can embed subjective perspectives. In our Data Equity Plan, the L&R Team has adopted theData Equity Framework by data equity consultants at We All Count that helps identify and address these equity-impacting choices across seven stages: funding, motivation, project design, data collection, analysis, interpretation, and communication. This systematic approach ensures that data projects reflect diverse experiences and achieve specified equity goals."
  },
  {
    "objectID": "blog/05_advancing_data_equity/index.html#our-progress-thus-far",
    "href": "blog/05_advancing_data_equity/index.html#our-progress-thus-far",
    "title": "Advancing Data Equity: Progress and Future Strategies",
    "section": "Our Progress Thus Far",
    "text": "Our Progress Thus Far\nIn FY24, the L&R team made strides in prioritizing data equity in our work. Specifically, we achieved several key goals of our Data Equity Plan:\n\nSeeking & Integrating Stakeholder Feedback to Revise Survey Instruments\n\nWe solicited feedback on our program evaluation plan from two stakeholder groups: our Teacher Advisory Board and the Programmatic Research & Evaluation Partnership (PREP) working group. TAB members provided valuable firsthand insight as educators on our Theory of Evaluation, as well as the survey-taking experience. Additionally, we received feedback from our colleagues on the PREP working group around the constructs on our Educator Survey. Feedback gathered from both stakeholder groups is currently being used to modify and refine our surveys for FY25, ensuring our surveys are more effective and relevant to our unique partnerships. The full report of stakeholder feedback can be accessed here.¬†\n\nImproved Student Survey Accessibility: Recognizing the diverse linguistic needs of our student population, we have made several enhancements to ensure our surveys are more accessible and comprehensible to all students. All student surveys have been translated into Spanish, with select student surveys additionally being available in French and Arabic (students have the option to self-select their preferred language). We also added visual indicators (1-Strongly disagree üëéüëé, 2-Somewhat disagree üëé, 3-Neither agree nor disagree üòê, 4-Somewhat agree üëç, 5-Strongly agree üëçüëç) for student survey response options where applicable, making it easier for students to understand response choices and accurately complete the survey.¬†\nEvaluating Our Motivations: At our FY24 in-person retreat, we discussed and defined our implicit and explicit motivations that impact our program evaluation work, such as alignment with programmatic implementation, data reporting timelines, organizational & individual goals, funding sources, and business development goals. From this conversation, we achieved a greater understanding and awareness amongst our team of the impacts these motivations have on our data collection and reporting processes.\nData Equity Education: As of FY24, five L&R team members have engaged in the Foundations of Data Equity Course from We All Count, meaning over half of our team has received formal training on the fundamental cornerstones of data equity."
  },
  {
    "objectID": "blog/05_advancing_data_equity/index.html#our-future-goals",
    "href": "blog/05_advancing_data_equity/index.html#our-future-goals",
    "title": "Advancing Data Equity: Progress and Future Strategies",
    "section": "Our Future Goals",
    "text": "Our Future Goals\nLooking ahead to FY25, the L&R team is committed to continuing our data equity work through the following actions:\n\nSharing Data Insights with Educators: We plan to explore and develop methods for sharing highlights and trends with participants that may be useful to their practice or support their growth, such as working with coaches to understand what data can and should be uplifted to teachers and how, in addition to exploring how to directly share information with teachers.¬†\nIncreasing Transparency: We aim to increase transparency with stakeholders regarding the motivations that influence our work. This includes clarifying how and why certain decisions are made throughout our data projects.\nOngoing Training: We will continue to develop our team‚Äôs knowledge of data equity through continuous training and learning opportunities. This will involve both formal courses and informal learning sessions, ensuring that our team stays updated on the latest best practices and research in data equity."
  },
  {
    "objectID": "blog/03_what_is_nps/index.html",
    "href": "blog/03_what_is_nps/index.html",
    "title": "The Basics: What is NPS?",
    "section": "",
    "text": "The Net Promoter Score (NPS) first appeared in the Harvard Business Review in 2003, and has since become the prominent and crucial metric for measuring the customer experience and predicting business success. It is, however, not without critique and can often be confusing. In this blog post, we will explore the reasons Teaching Lab uses NPS, its benefits, and its critiques.¬†\nNPS‚Äôs are measured with a single question and reported with a number from the range -100 to +100, a higher score being desirable.\nThe NPS question is framed as:\n\nHow likely is it that you would recommend our company/product/service to a friend or colleague?\n\ni.e.¬†How likely are you to promote us?\nNotice that the question is explicitly not about whether the customer is happy, but about whether the product (in Teaching Lab‚Äôs case, service) is something they would attach their own name and credibility to by recommending it to someone they personally know. By this methodology, NPS indicates not only customer satisfaction but customer loyalty. The former indicates attitude, and the latter is behavioral, which has much more remunerative implications.¬†\nRespondents answer the question by choosing a number on a scale of 0-10, with 10 meaning they are ‚Äúextremely likely‚Äù to recommend something.¬†\n\nRespondents are divided into three groups as shown in the above graphic: Promoters, Passives, and Detractors based on their answers.\n\nPromoters: Promoters are likely to enthusiastically recommend your company or service. As stated in the Harvard Business Review: ‚ÄúThe only path to profitable growth may lie in a company‚Äôs ability to get its loyal customers to become, in effect, its marketing department.‚Äù\nPassives: Passives are probably satisfied but not as enthusiastic about the company or service compared to a promoter. If another company offers an excellent option, these customers might choose it, as they are not as loyal to you. On a scale of 1-10, their answers are 7 or 8.\nDetractor: Detractors can destroy a company with negative reviews. Whether they had an adverse experience or didn‚Äôt like the font you use, detractors will likely post about how much they do not like your business. On the other hand, every detractor represents a missed opportunity to add a promoter.\n\nFinally, the answers from all individual customers are then aggregated into one single number, a single company statistic, which is the company‚Äôs Net Promoter Score.\n\nAgain, the NPS can range from -100 to 100.¬†\nOnce calculated, what is a good NPS?\nThis question is tougher to answer than you might think. Technically, any score above 0 can be considered a ‚Äúgood‚Äù score, since it means you have more Promoters than Detractors. According to global NPS standards, a score of above 50 is good, and above 70 is outstanding. Survey Monkey‚Äôs global benchmark of 150,000 organizations finds that the average NPS is 32.\nAnd importantly, ‚Äúgood NPS‚Äù is industry-specific. In a Qualtrics blog post by Adam Lang that reviews 2020 data for automakers, software, and more, the following ranges were found to be standard for each industry:\n\nTo further complicate things, the NPS can change dramatically given the number of responses. Here is a fun animation from Qualtrics showing how the overall NPS changes as responses come in, giving great insight into the ultimate number. (Note: This is not Teaching Lab data.)¬†\n\n\nHistory, controversy, applicability, alternatives\nNet Promoter Score rose to prominence after the release of a 2003 Harvard Business Review article titled ‚ÄúThe One Number You Need to Grow.‚Äù The overarching message of the article is that the measurement of customer satisfaction and customer retention does not help firms achieve growth; instead, word of mouth is the metric that is linked to growth. In the foundational study for the article by Bain & Company, 400 companies across more than a dozen industries collected data, and then correlations were run comparing NPS to each company‚Äôs revenue growth rate (Satmetrix, 2004).¬†\nThe attention-grabbing headline has, however, had its fair share of critique. Subsequent scholarly research has found that ‚ÄúNet Promoter in no way would be categorized as the single most reliable indicator of a company‚Äôs ability to grow‚Äú (Keiningham, 2007). Bain & Company study author Fred Reichheld has even acknowledged its imperfections in the analytics that were used to support Net Promoter:¬†\n‚ÄúAll we did was quantify this common sense in a way that made sense to business leaders‚Äîthe target audience for my book. These practical leaders have little interest in advanced statistical methods. Frankly, we see little value in continued debate about cause versus correlation, timeframes, or statistical methods.‚Äù\nFurther studies have found that actual consumer behavior (recommendation giving) does not necessarily reflect the uniform idea of the question (HBR, 2019), suggesting that human complexity throws yet more issues into the calculation. Another paper, ‚ÄúMeasuring Customer Satisfaction and Loyalty: Improving the ‚ÄòNet-Promoter‚Äô Score‚Äù has even counterintuitively demonstrated that ‚Äúsatisfaction‚Äù and ‚Äúliking‚Äù are better predictors of recommendations than ‚Äúlikelihood to recommend‚Äù (Schneider, 2008). We can actually even see the potential for this likelihood in Teaching Lab‚Äôs data.\n\nThe above figure shows the relationship between NPS and facilitation feedback where each ‚Äústrongly agree‚Äù has been recoded to a 5, ‚Äúagree‚Äù to a 4, and so on until ‚Äústrongly disagree‚Äù is a 1. The data clearly demonstrates that participants can enjoy the facilitation or coaching, but still give a ‚Äúpassive‚Äù rating. Additionally, a clear upward diagonal pull in the local polynomial regression model implemented shows that Net Promoter Scores occur at the highest frequency in the 7, 8, 9, and 10 groups, with 5‚Äôs across the board being crucial to get a 9 or 10 score. Unfortunately, the 7 and 8 scores are discounted in the NPS calculation. It can at least be clearly stated that NPS is not the only metric to rely on, as promised by Bain & Company.¬†\nHidden behind a low NPS can often be a lot of passive (7 or 8) ratings - check out one of Teaching Lab‚Äôs current NPS‚Äôs; 31.55% of people giving a 7 or 8 rating can really impact the ultimate, overall NPS.\n\n\n\nFrom Ongoing Report, March 12\n\n\nThis is the essence of NPS: even though this number might be lower than our goal, it does not necessarily mean people didn‚Äôt like the service; instead, it is intended to measure whether or not they would advocate for the service, something that is much harder to achieve. Think about it: How often do you advocate for a product?\nInterestingly, there is a big difference in Teaching Lab‚Äôs NPS, depending on the PL audience!\nFor teachers, the NPS is currently 33.12, which is the lowest among all the PL audience types.\n\n\n\nFrom Ongoing Report, March 12\n\n\nAnd for contact leads, we have the highest overall NPS!\n\n\n\nFrom Ongoing Report, March 12\n\n\nThese bar charts demonstrate the range of the reality of what is being measured - certain people might be more likely to recommend a service in the first place. Additionally note what is impacting the NPS the most here - ‚Äúother educator‚Äù and ‚Äúteacher‚Äù have about the same percent of passive scores, but teachers tend to be detractors (0-6 rating) more often, whereas contact leads in general are more likely to be promoters, and less likely to be either detractors or passive.\n\n\nPredict your NPS\nUse the following model and parameters to predict what your next NPS rating will be! Note that this predictive model is by no means a guarantee, but it predicted today‚Äôs NPS scores with 82% accuracy!\n\n\n\n\n\n\nThe argument for continuing to use NPS\n\nIt‚Äôs a leading indicator of business growth.\n\nThe logic follows that the more enthusiastic customers you have, the more new customers you can attract (and with less effort or investment), and this cycle becomes a growth engine.\n\nIt‚Äôs simpler and more productive than traditional customer satisfaction surveys.\n\nNPS is low friction on both sides, it is easy to evaluate and easy to answer.\n\nIt clearly shows where pain points are on the aggregate.\n\nNPS results show strengths and weaknesses in the customer base, identifying where teams can work to improve customer relations.\n\n\n\n\nThe argument against continuing to use NPS\n\nIt doesn‚Äôt accurately differentiate promoters and detractors.\n\nThe rule-of-thumb score classifications proposed by Reichheld (promoters are those respondents who give a likelihood of recommendation of 9 or 10 while the detractors give 6 or less) could be further refined.¬† This approach may not fully capture the statistical nuances between different respondent groups.¬† Additionally, it might obscure significant trends and lead to misunderstandings about the Net Promoter Score (NPS), suggesting a negative score when the actual scenario might be different.\n\nIt can be used incorrectly.\n\nA common criticism is that companies send the survey too frequently. There are eventually diminishing returns on customer surveys. Results become less representative when survey fatigue starts to set in.¬†\n\nIf it is used as the only measure.\n\nThe other common criticism is that companies rely too heavily on NPS alone. They don‚Äôt use any other measurements of customer experience and satisfaction to complement the Net Promoter Score. To note: Teaching Lab has comprehensive post-session and post-PL Series participant feedback surveys, in addition to the contact lead survey. NPS is one of many measures for our impact on teaching and learning.¬†\n\nCustomer feedback experts have diverse opinions on its applicability across all scenarios.\n\nWhen asked to compare the NPS with other loyalty indices, again, only 19% of the customer feedback professionals agreed that the NPS is a better predictor of growth compared to other loyalty indices. The remaining 81% of the customer feedback professionals either disagreed with (~40%) or remained neutral toward (~40%) the statements regarding the merits of the NPS (Hayes, 2008).\n\n\n\n\nNo metric is perfect\n‚ÄúAll models are wrong, some are useful‚Äù - George Box (British statistician). A statement that is especially true for the less than straightforward NPS metric can be shown to be wrong, but if wielded well, it can be useful. Most research suggests sending the surveys frequently, but to different subsets of customers, at regular milestones in the customer lifecycle. Using this methodology you can pivot and tweak your service, rather than getting a static feedback dump every quarter. Teaching Lab adheres to this practice, including it in participant feedback surveys rather than only doing it quarterly or otherwise infrequently.\nLastly, using a customer loyalty index with other questions relevant to your business can be essential in finding specific issues and highlights in customer feedback. Teaching Lab also does this; our participant feedback surveys incorporate 5-point Likert-scale items as well as open-ended items in conjunction with the NPS to provide a more holistic view of participants‚Äô experiences and opinions.\n\nWhat does this mean for how I interpret feedback data for my partner?\nIn wrapping up our analysis, it‚Äôs crucial to not only consider NPS but also to delve into other vital metrics that offer a broader view of customer satisfaction and loyalty Recently, at Teaching Lab, we‚Äôve introduced targeted qualitative questions based on the respondents‚Äô NPS category to enrich our understanding and add depth to our quantitative data. Detractors are optionally prompted with, ‚ÄúWe‚Äôre sorry to hear that. What would you like us to improve on?‚Äù to identify areas for enhancement. Passive respondents are optionally prompted with ‚ÄúThank you for your feedback. Would you like to tell us why you would score us that way?‚Äù to uncover underlying reasons for their neutrality. Lastly, promoters are asked, ‚ÄúWe‚Äôre glad you like us. Would you like to tell us what exactly excites you?‚Äù to highlight strengths and areas of excellence. This strategic addition aims to provide a more nuanced view of our customer experience, allowing us to tailor our strategies more effectively to meet and exceed customer expectations. To see this new data (and more on participant feedback), head to the Facilitator/Coaching Dashboard and check out the NPS rollup!\n\n\n\nFrom Ongoing Report, March 12\nFrom Ongoing Report, March 12\nFrom Ongoing Report, March 12"
  },
  {
    "objectID": "dashboard/participant_feedback_3.html",
    "href": "dashboard/participant_feedback_3.html",
    "title": "Participant Feedback Qualtrics Dashboard",
    "section": "",
    "text": "Go to the qualtrics dashboard ‚û°\n\nFor instructions on how to login to the qualtrics dashboard see the linked document.\n&lt;iframe src=‚Äúhttps://docs.google.com/document/d/e/2PACX-1vRTxmNmVV9dR6jguGB8M3GFn8w1xGj8YFMkPkj52venVC9qRCQ14zqBJHt37ggoptUKpYIvFEG0JI3D/pub?embedded=true‚Äù style=‚Äúwidth: 1000px; border: none; margin: 0; height: 800px;‚Äù"
  },
  {
    "objectID": "dashboard/participant_feedback_1.html",
    "href": "dashboard/participant_feedback_1.html",
    "title": "Participant Feedback // Ongoing Report",
    "section": "",
    "text": "Go to section 1 of the ongoing report ‚û°Ô∏è"
  },
  {
    "objectID": "people/duncan_gates.html",
    "href": "people/duncan_gates.html",
    "title": "Duncan Gates",
    "section": "",
    "text": "Duncan Gates is a data analyst/scientist with research interests in education, economics, computational modeling, and AI."
  },
  {
    "objectID": "people/duncan_gates.html#education",
    "href": "people/duncan_gates.html#education",
    "title": "Duncan Gates",
    "section": "Education",
    "text": "Education\nH.B.S, Oregon State University - 2020 (Economics, Math)"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "Open Source License",
    "section": "",
    "text": "TL Software 0.1 (and earlier) is licensed under the GNU GPL v2. Quarto version 1.4 is licensed under the MIT License. We believe that it‚Äôs better for everyone if the tools used for research and science are free and open. Reproducibility, widespread sharing of knowledge and techniques, and the leveling of the playing field by eliminating cost barriers are but a few of the shared benefits of free software in science."
  },
  {
    "objectID": "Data_dives.html#beyond-the-basics-mastering-the-data-collection-lifecycle-in-partner-projects",
    "href": "Data_dives.html#beyond-the-basics-mastering-the-data-collection-lifecycle-in-partner-projects",
    "title": "Data Dives",
    "section": "Beyond the Basics: Mastering the Data Collection Lifecycle in Partner Projects",
    "text": "Beyond the Basics: Mastering the Data Collection Lifecycle in Partner Projects\nExploring the data collection lifecycle in partner projects. Zoom recording here."
  },
  {
    "objectID": "Data_dives.html#evaluating-excellence-insights-into-our-student-work-analysis",
    "href": "Data_dives.html#evaluating-excellence-insights-into-our-student-work-analysis",
    "title": "Data Dives",
    "section": "Evaluating Excellence: Insights into Our Student Work Analysis",
    "text": "Evaluating Excellence: Insights into Our Student Work Analysis\nA demonstration of how course assessments work to evaluate teacher content knowledge, and a dive into the student work graded dashboard which has the ability to explore aggregate data and individual student work samples! Zoom recording here."
  },
  {
    "objectID": "Data_dives.html#course-assessments-how-we-know-were-increasing-participant-knowledge",
    "href": "Data_dives.html#course-assessments-how-we-know-were-increasing-participant-knowledge",
    "title": "Data Dives",
    "section": "Course Assessments: How We Know We‚Äôre Increasing Participant Knowledge",
    "text": "Course Assessments: How We Know We‚Äôre Increasing Participant Knowledge\nA demonstration of how course assessments work to evaluate teacher content knowledge, and a dive into the course assessment dashboard which has many fun levels of data! Zoom recording here."
  },
  {
    "objectID": "Data_dives.html#behind-the-numbers-what-we-measure-why",
    "href": "Data_dives.html#behind-the-numbers-what-we-measure-why",
    "title": "Data Dives",
    "section": "Behind the Numbers: What We Measure & Why",
    "text": "Behind the Numbers: What We Measure & Why\nA thorough review of why we use the Guskey Framework, specifically what does Teaching Lab‚Äôs data say, and what our practice looks like. Zoom recording here."
  }
]